//
//  SpeechRecognitionManager.swift
//  Talk_AI_Medicalassitant_3
//

import Speech
import AVFoundation
import Combine
import AudioToolbox

class SpeechRecognitionManager: ObservableObject {
    // MARK: - Published Properties
    @Published var transcribedText: String = ""
    @Published var isRecording: Bool = false
    @Published var error: Error? = nil
    @Published var displayError: DisplayError? = nil
    
    // MARK: - Public Properties
    let autoSendTrigger = PassthroughSubject<String, Never>()
    
    // MARK: - Private Properties
    private let audioEngine = AVAudioEngine()
    private var recognitionTask: SFSpeechRecognitionTask?
    private let speechRecognizer: SFSpeechRecognizer?
    private var recognitionRequest: SFSpeechAudioBufferRecognitionRequest?
    
    // éŸ³å£°èªè­˜ã®å®‰å®šæ€§ã‚’é«˜ã‚ã‚‹ãŸã‚ã®ãƒ—ãƒ­ãƒ‘ãƒ†ã‚£
    private let bufferSize: AVAudioFrameCount = 1024 * 4
    var currentText: String = ""
    
    // ã‚¿ã‚¤ãƒãƒ¼é–¢é€£
    private var silenceTimer: Timer?
    private let silenceInterval: TimeInterval = 2.5
    private var lastTranscriptionTime: Date?
    
    // çŠ¶æ…‹ç®¡ç†
    var hasAutoSent: Bool = false
    
    // MARK: - Initialization
    init(locale: Locale = Locale(identifier: "ja-JP")) {
        self.speechRecognizer = SFSpeechRecognizer(locale: locale)
    }
    
    // Viewãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«æ§‹ç¯‰æ™‚ã«åˆæœŸåŒ–ã—ã¦ç”Ÿæˆã•ã‚Œã‚‹å¯¾ç­–
    func prepare() {
        SFSpeechRecognizer.requestAuthorization { [weak self] status in
            DispatchQueue.main.async {
                switch status {
                case .authorized:
                    self?.speechRecognizer?.delegate = self as? SFSpeechRecognizerDelegate
                case .denied:
                    self?.displayError = DisplayError(message: "éŸ³å£°èªè­˜ã®æ¨©é™ãŒæ‹’å¦ã•ã‚Œã¦ã„ã¾ã™", isCancelable: true, isMaintenance: true)

                case .restricted:
                    self?.displayError = DisplayError(message: "ã“ã®ç«¯æœ«ã§ã¯éŸ³å£°èªè­˜ãŒåˆ¶é™ã•ã‚Œã¦ã„ã¾ã™", isCancelable: true)

                case .notDetermined:
                    self?.displayError = DisplayError(message: "éŸ³å£°èªè­˜ã®æ¨©é™ãŒæœªè¨­å®šã§ã™", isCancelable: true, isMaintenance: true)

                @unknown default:
                    break
                }
            }
        }
    }
    
    // MARK: - Public Methods
    func startRecording() {
        debugLog("éŒ²éŸ³é–‹å§‹å‡¦ç†ã‚’é–‹å§‹")
        hasAutoSent = false
        resetSilenceTimer()
        resetRecognition()
        currentText = ""
        
        // éŸ³ã‚’é³´ã‚‰ã™
        playSound(id: 1256)
        
        do {
            try configureAudioSession()
        } catch {
            self.displayError = DisplayError(message: R.string.errorMessageSpeechRecognition, isCancelable: false)
            return
        }
        
        recognitionRequest = SFSpeechAudioBufferRecognitionRequest()
        guard let recognitionRequest = recognitionRequest else {
            self.displayError = DisplayError(message: "éŸ³å£°èªè­˜ãƒªã‚¯ã‚¨ã‚¹ãƒˆã®ä½œæˆã«å¤±æ•—ã—ã¾ã—ãŸ", isCancelable: false)
            return
        }
        
        recognitionRequest.shouldReportPartialResults = true
        recognitionRequest.taskHint = .dictation
        
        let inputNode = audioEngine.inputNode
        let recordingFormat = inputNode.outputFormat(forBus: 0)
        
        inputNode.installTap(onBus: 0, bufferSize: bufferSize, format: recordingFormat) { [weak self] buffer, _ in
            self?.recognitionRequest?.append(buffer)
        }
        
        recognitionTask = speechRecognizer?.recognitionTask(with: recognitionRequest) { [weak self] result, error in
            guard let self = self else { return }
            
            if let errorDetail: NSError = error as NSError? {
                // ä¸€éƒ¨ã®ã‚¨ãƒ©ãƒ¼ã¯ã‚¹ãƒ«ãƒ¼
                if !(errorDetail.code == 301 || errorDetail.code == 216 || errorDetail.code == 1110) {
                    self.displayError = DisplayError(message: R.string.errorMessageSpeechRecognition, isCancelable: false)
                    return
                }
                
            }
            
            if let result = result {
                DispatchQueue.main.async {
                    let newText = result.bestTranscription.formattedString
                    self.transcribedText = newText
                    /*
                    // æ—¢å­˜ã®ãƒ†ã‚­ã‚¹ãƒˆã¨æ–°ã—ã„èªè­˜çµæœã‚’çµåˆ
                    if !self.currentText.isEmpty {
                        self.transcribedText = "\(self.currentText) \(newText)"
                    } else {
                        self.transcribedText = newText
                    }
                    */
                    self.debugLog("æ–°ã—ã„ãƒ†ã‚­ã‚¹ãƒˆèªè­˜: \(newText)")
                    self.handleNewTranscription()
                }
            }
        }
        
        audioEngine.prepare()
        
        do {
            try audioEngine.start()
        } catch {
            self.displayError = DisplayError(message: R.string.errorMessageSpeechRecognition, isCancelable: false)
            return
        }
        
        isRecording = true
        debugLog("éŒ²éŸ³é–‹å§‹å®Œäº†")
    }
    
    func stopRecording() -> String {
        if (!isRecording) {
            return ""
        }
        debugLog("éŒ²éŸ³åœæ­¢å‡¦ç†ã‚’é–‹å§‹")
        resetSilenceTimer()
        audioEngine.stop()
        audioEngine.inputNode.removeTap(onBus: 0)
        recognitionRequest?.endAudio()
        recognitionTask?.cancel()
        
        playSound(id: 1255)
        isRecording = false
        let finalText = transcribedText
        
        // è‡ªå‹•é€ä¿¡ã•ã‚Œã¦ã„ãªã„å ´åˆã®ã¿ã€æœ€çµ‚ãƒ†ã‚­ã‚¹ãƒˆã‚’è¿”ã™
        if !hasAutoSent && !finalText.isEmpty {
            debugLog("æ‰‹å‹•åœæ­¢ã«ã‚ˆã‚‹é€ä¿¡ãƒ†ã‚­ã‚¹ãƒˆ: \(finalText)")
            return finalText
        }
        debugLog("é€ä¿¡ãƒ†ã‚­ã‚¹ãƒˆãªã—ï¼ˆè‡ªå‹•é€ä¿¡æ¸ˆã¿ã¾ãŸã¯ç©ºï¼‰")
        return ""
    }
    
    func clearText() {
        transcribedText = ""
        hasAutoSent = false
        debugLog("ãƒ†ã‚­ã‚¹ãƒˆã¨ãƒ•ãƒ©ã‚°ã‚’ã‚¯ãƒªã‚¢")
    }
    
    func acceptError() {
        self.displayError = nil
    }
    
    // MARK: - Private Methods
    private func handleNewTranscription() {
        lastTranscriptionTime = Date()
        resetSilenceTimer()
        
        // æ–°ã—ã„ãƒ†ã‚­ã‚¹ãƒˆãŒã‚ã‚‹å ´åˆã®ã¿ã‚¿ã‚¤ãƒãƒ¼ã‚’è¨­å®š
        if !transcribedText.isEmpty {
            silenceTimer = Timer.scheduledTimer(withTimeInterval: silenceInterval, repeats: false) { [weak self] _ in
                self?.handleSilenceTimeout()
            }
            debugLog("ç„¡éŸ³æ¤œå‡ºã‚¿ã‚¤ãƒãƒ¼ã‚’è¨­å®š: \(silenceInterval)ç§’")
        }
    }
    
    private func handleSilenceTimeout() {
        guard !hasAutoSent && !transcribedText.isEmpty else {
            debugLog("è‡ªå‹•é€ä¿¡ã‚’ã‚¹ã‚­ãƒƒãƒ—ï¼ˆæ—¢ã«é€ä¿¡æ¸ˆã¿ã¾ãŸã¯ç©ºãƒ†ã‚­ã‚¹ãƒˆï¼‰")
            return
        }
        
        hasAutoSent = true
        debugLog("ç„¡éŸ³æ¤œå‡ºã«ã‚ˆã‚‹è‡ªå‹•é€ä¿¡: \(transcribedText)")
        playSound(id: 1004) // è‡ªå‹•é€ä¿¡æ™‚ã«éŸ³ã‚’é³´ã‚‰ã™
        autoSendTrigger.send(transcribedText)
        
        resetSilenceTimer()
    }
    
    private func resetSilenceTimer() {
        silenceTimer?.invalidate()
        silenceTimer = nil
        debugLog("ç„¡éŸ³æ¤œå‡ºã‚¿ã‚¤ãƒãƒ¼ã‚’ãƒªã‚»ãƒƒãƒˆ")
    }
    
    private func configureAudioSession() throws {
        let audioSession = AVAudioSession.sharedInstance()
        do {
            try audioSession.setCategory(.playAndRecord, mode: .default, options: [.mixWithOthers, .defaultToSpeaker])
            try audioSession.setActive(true)
        } catch {
            debugLog("Audio session setup error: \(error)")
        }
        debugLog("ã‚ªãƒ¼ãƒ‡ã‚£ã‚ªã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’è¨­å®š")
    }
    
    private func resetRecognition() {
        recognitionTask?.cancel()
        recognitionTask = nil
        recognitionRequest = nil
    }
    
    private func playSound(id: SystemSoundID) {
        AudioServicesPlaySystemSound(id)
    }
    
    private func debugLog(_ message: String) {
        if Constants.devMode {
            print("ğŸ¤ [SpeechRecognition] \(message)")
        }
    }
}

